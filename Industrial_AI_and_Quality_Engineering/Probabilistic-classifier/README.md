
This project is a simple but powerful introduction to 'probabilistic classification' using the 'Naive Bayes algorithm'.  
The goal is to understand how Bayes’ Theorem is applied in classification tasks and to compute manually:

- Prior probabilities  
- Conditional probabilities  
- Posterior probabilities  
- Final classification decision  

This is part of my broader exploration into 'probability models', 'machine learning fundamentals', and 'statistical decision-making'.

---

Project Overview

Using a classic example dataset (“Play Tennis”), the project predicts whether tennis should be played given four categorical weather conditions:

- 'Outlook' (Sunny, Overcast, Rain)  
- 'Temperature' (Hot, Mild, Cool)  
- 'Humidity' (High, Normal)  
- 'Wind' (Weak, Strong)

The target variable is:

- 'PlayTennis' (Yes / No)

The implementation computes the Naive Bayes decision 'from scratch' — without any ML libraries — to deepen understanding of the math behind classification.

---

This Project Demonstrates

- How 'Bayes’ Theorem' works in classification  
- How to compute:
  - 'Prior probabilities'  
  - 'Conditional probabilities' 
  - 'Posterior probabilities'  
  - 'Normalised probabilities'  
- How Naive Bayes assumes feature independence  
- How classification decisions follow the highest probability  
- How categorical variables can be modelled probabilistically  

This is an essential foundation before moving on to more advanced models.

---

Learning Outcomes

Through this project, I gained hands-on understanding of:

- How probability theory forms the basis of ML classification
- How categorical data can be modelled with likelihoods
- Why Naive Bayes works well despite the independence assumption
- How to calculate posterior probabilities directly from data
- The importance of normalisation and avoiding data leakage
